{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9420614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/porterhunley/opt/anaconda3/envs/mediresearch/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "import os\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "from sqlalchemy import create_engine\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7a86f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    device = torch.device('cpu')\n",
    "    if (torch.cuda.is_available()):\n",
    "        device = torch.device(\"cuda\")\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "DEVICE = get_device()\n",
    "\n",
    "\n",
    "tag2idx = {\n",
    "    'B': 2,\n",
    "    'I': 0,\n",
    "    'O': 1,\n",
    "    'PAD': 3\n",
    "}\n",
    "idx2tag = {v: k for k,v in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bc1eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_from_string(string, model, tokenizer):\n",
    "    tokenized_sentence = tokenizer.encode(string)\n",
    "    print(len(tokenized_sentence))\n",
    "    print(string)\n",
    "    input_ids = torch.tensor([tokenized_sentence]).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "\n",
    "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "\n",
    "    # Join the split tokens\n",
    "    new_tokens, new_labels = [], []\n",
    "    for token, label_idx in zip(tokens, label_indices[0]):\n",
    "        if token.startswith(\"##\"):\n",
    "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "\n",
    "        else:\n",
    "            new_labels.append(idx2tag[label_idx])\n",
    "            new_tokens.append(token)\n",
    "\n",
    "    return new_tokens, new_labels\n",
    "\n",
    "\n",
    "def get_unique_treatments(string, model, tokenizer):\n",
    "    tokens, labels = get_ner_from_string(string, model, tokenizer)\n",
    "    treatments = []\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        token = tokens[i]\n",
    "        label = labels[i]\n",
    "\n",
    "    if label == 'B':\n",
    "        treatments.append(token)\n",
    "\n",
    "    if label == 'I' and treatments:\n",
    "        treatments[-1] += token\n",
    "  \n",
    "    return list(set(treatments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a047f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups(connection):\n",
    "    print(\"Reading groups...\")\n",
    "    groups = pd.read_sql(\"select id, title, description from temp_schema.groups\", connection)\n",
    "    groups['text'] = 'Title: ' + groups['title'] + ' Description: ' + groups['description']\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def get_treatments(connection):\n",
    "    print(\"Reading treatments...\")\n",
    "    treats = pd.read_sql('select id as treat_id, name as treatments from temp_schema.treatments', connection)\n",
    "\n",
    "    return treats \n",
    "\n",
    "\n",
    "def get_effect_groups(connection):\n",
    "    print(\"Reading effects groups...\")\n",
    "    effect_groups = pd.read_sql(\"select id, title, description from temp_schema.effectsgroups\", connection)\n",
    "    effect_groups['text'] = 'Title: ' + effect_groups['title'] + ' Description: ' + effect_groups['description']\n",
    "\n",
    "    return effect_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54b2b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer():\n",
    "    print(\"Loading tokenizer...\")\n",
    "    return BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8903843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ner_model():\n",
    "    print(\"Loading NER model...\")\n",
    "    model_path = f'{MODEL_PATH}/ClinicalBertNERModel.pt'\n",
    "    if (not os.path.exists(model_path)):\n",
    "        download_model()\n",
    "\n",
    "    model = BertForTokenClassification.from_pretrained(\n",
    "        \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "        num_labels=len(tag2idx),\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b68d83dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_treatments(groups: pd.DataFrame, effect_groups: pd.DataFrame):\n",
    "    # TODO - this should be batched\n",
    "    model = load_ner_model()\n",
    "    tokenizer = load_tokenizer()\n",
    "\n",
    "    tqdm.pandas()\n",
    "\n",
    "    groups['treatments'] = groups['text'].progress_apply(lambda x: get_unique_treatments(x, model, tokenizer))\n",
    "    print(groups['treatments'])\n",
    "    effect_groups['treatments'] = effect_groups['text'].progress_apply(lambda x: get_unique_treatments(x, model, tokenizer))\n",
    "\n",
    "    unique_treatments = pd.concat([groups['treatments'], effect_groups['treatments']], axis=0)\\\n",
    "        .explode('treatments')\\\n",
    "        .str.lower()\\\n",
    "        .drop_duplicates()\\\n",
    "        .to_frame()\n",
    "\n",
    "    unique_treatments = unique_treatments.rename(columns={ 'treatments': 'name'})\n",
    "    unique_treatments['id'] = range(1, len(unique_treatments) + 1)\n",
    "    unique_treatments = unique_treatments.reset_index(drop=True)\n",
    "\n",
    "    return unique_treatments, groups, effect_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb0502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_URL = os.environ.get('DATABASE_URL', default=\"postgresql://meditreats@localhost:5432\")\n",
    "MODEL_PATH = os.environ.get('MODEL_PATH', default=\"/Users/porterhunley/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "983b92ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "BATCH_SIZE=100\n",
    "\n",
    "def get_ner_from_batch(strings, model, tokenizer):\n",
    "    tokenized_sentences = [tokenizer.encode(s) for s in tqdm(strings)]\n",
    "    input_ids = pad_sequence([torch.tensor(tokens) for tokens in tokenized_sentences], batch_first=True, padding_value=tokenizer.pad_token_id).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "    \n",
    "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "    batch_size, seq_len = label_indices.shape\n",
    "\n",
    "    tokens_batch = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy())\n",
    "\n",
    "    new_tokens_batch, new_labels_batch = [], []\n",
    "    for i in range(batch_size):\n",
    "        new_tokens, new_labels = [], []\n",
    "        for j in range(seq_len):\n",
    "            token, label_idx = tokens_batch[i][j], label_indices[i][j]\n",
    "            if token.startswith(\"##\") and new_tokens:\n",
    "                new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                new_labels.append(idx2tag[label_idx])\n",
    "                new_tokens.append(token)\n",
    "        new_tokens_batch.append(new_tokens)\n",
    "        new_labels_batch.append(new_labels)\n",
    "\n",
    "    return new_tokens_batch, new_labels_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b7bc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_treatments_b(groups: pd.DataFrame, effect_groups: pd.DataFrame):\n",
    "    # TODO - this should be batched\n",
    "    tqdm.pandas()\n",
    "\n",
    "    text_batches = [groups['text'][:500].values, effect_groups['text'][:500].values]\n",
    "    model = load_ner_model()\n",
    "    tokenizer = load_tokenizer()\n",
    "\n",
    "    tqdm.pandas()\n",
    "\n",
    "    treatments_batches = []\n",
    "    for text_batch in text_batches:\n",
    "        tokens_batch, _ = get_ner_from_batch(text_batch, model, tokenizer) #25000 text values\n",
    "        treatments_batch = [set(t) for t in tokens_batch]\n",
    "        treatments_batches.append(treatments_batch)\n",
    "\n",
    "    groups['treatments'] = [list(t) for t in treatments_batches[0]]\n",
    "    effect_groups['treatments'] = [list(t) for t in treatments_batches[1]]\n",
    "\n",
    "    unique_treatments = pd.concat([groups['treatments'], effect_groups['treatments']], axis=0)\\\n",
    "        .explode('treatments')\\\n",
    "        .str.lower()\\\n",
    "        .drop_duplicates()\\\n",
    "        .to_frame()\n",
    "\n",
    "    unique_treatments = unique_treatments.rename(columns={ 'treatments': 'name'})\n",
    "    unique_treatments['id'] = range(1, len(unique_treatments) + 1)\n",
    "    unique_treatments = unique_treatments.reset_index(drop=True)\n",
    "\n",
    "    return unique_treatments, groups, effect_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "645ff36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"postgresql://meditreats:meditreats@localhost:5432\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3c8dcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading groups...\n",
      "Reading effects groups...\n",
      "Loading NER model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 1924.13it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m groups \u001b[38;5;241m=\u001b[39m get_groups(connection)\n\u001b[1;32m      4\u001b[0m effect_groups \u001b[38;5;241m=\u001b[39m get_effect_groups(connection)\n\u001b[0;32m----> 6\u001b[0m treats, treat_groups, treat_effect_groups \u001b[38;5;241m=\u001b[39m parse_treatments_b(groups, effect_groups)\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mparse_treatments_b\u001b[0;34m(groups, effect_groups)\u001b[0m\n\u001b[1;32m     11\u001b[0m treatments_batches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_batch \u001b[38;5;129;01min\u001b[39;00m text_batches:\n\u001b[0;32m---> 13\u001b[0m     tokens_batch, _ \u001b[38;5;241m=\u001b[39m get_ner_from_batch(text_batch, model, tokenizer) \u001b[38;5;66;03m#25000 text values\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     treatments_batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mset\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens_batch]\n\u001b[1;32m     15\u001b[0m     treatments_batches\u001b[38;5;241m.\u001b[39mappend(treatments_batch)\n",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m, in \u001b[0;36mget_ner_from_batch\u001b[0;34m(strings, model, tokenizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m label_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     14\u001b[0m batch_size, seq_len \u001b[38;5;241m=\u001b[39m label_indices\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 16\u001b[0m tokens_batch \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(input_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     18\u001b[0m new_tokens_batch, new_labels_batch \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mediresearch/lib/python3.11/site-packages/transformers/tokenization_utils.py:906\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    904\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m--> 906\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index)\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "connection = engine.connect()\n",
    "\n",
    "groups = get_groups(connection)\n",
    "effect_groups = get_effect_groups(connection)\n",
    "\n",
    "treats, treat_groups, treat_effect_groups = parse_treatments_b(groups, effect_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8947cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
